# -*- coding: utf-8 -*-
"""Transfer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z9UlCNSiKkewjQlHCGvTBz8WRlPQCxa4
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# Load data
CSV_PATH = "/content/drive/My Drive/Amharic News Dataset.csv"

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class AmharicNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_and_prepare_data(csv_path):
    """Load and prepare the Amharic news dataset"""
    print("Loading dataset...")

    # Read the CSV file
    df = pd.read_csv(csv_path)

    # Show available columns
    available_columns = df.columns.tolist()
    print(f"Available columns: {available_columns}")

    # Try to automatically detect text and label columns
    text_candidates = ['text', 'article', 'content', 'title', 'body', 'amharic_text', 'news', 'description']
    label_candidates = ['label', 'category', 'class', 'topic', 'type', 'target']

    text_column = next((col for col in text_candidates if col in df.columns), available_columns[0])
    label_column = next((col for col in label_candidates if col in df.columns), available_columns[1])

    print(f"Using text column: '{text_column}'")
    print(f"Using label column: '{label_column}'")

    # Handle missing values
    texts = df[text_column].fillna('').astype(str).values
    labels = df[label_column].values

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class AmharicNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_and_prepare_data(csv_path):
    """Load and prepare the Amharic news dataset"""
    print("Loading dataset...")

    # Read the CSV file
    df = pd.read_csv(csv_path)

    # Show available columns
    available_columns = df.columns.tolist()
    print(f"Available columns: {available_columns}")

    # Use the correct column names from your dataset
    text_column = 'article'  # This contains the news text
    label_column = 'category'  # This contains the category labels

    print(f"Using text column: '{text_column}'")
    print(f"Using label column: '{label_column}'")

    # Handle missing values and clean data
    df[text_column] = df[text_column].fillna('').astype(str)
    df[label_column] = df[label_column].fillna('Unknown')

    # Convert all labels to strings to handle mixed types
    df[label_column] = df[label_column].astype(str)

    # Remove any rows with empty text
    df = df[df[text_column].str.strip() != '']

    texts = df[text_column].values
    labels = df[label_column].values

    # Show some statistics about the data
    print(f"\nDataset statistics:")
    print(f"Total samples: {len(texts)}")

    # Calculate text length statistics safely
    text_lengths = [len(str(t)) for t in texts]
    print(f"Text length stats - Min: {min(text_lengths)}, "
          f"Max: {max(text_lengths)}, "
          f"Avg: {np.mean(text_lengths):.1f}")

    # Show unique categories
    unique_categories, category_counts = np.unique(labels, return_counts=True)
    print(f"\nUnique categories: {len(unique_categories)}")

    # Show top categories
    print("Category distribution (top 20):")
    for i, (category, count) in enumerate(zip(unique_categories, category_counts)):
        if i < 20:  # Show only top 20 to avoid too much output
            print(f"  {category}: {count} samples ({count/len(labels)*100:.1f}%)")
        elif i == 20:
            print("  ... (showing top 20 categories only)")

    # Convert labels to numerical values
    le = LabelEncoder()
    labels = le.fit_transform(labels)
    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    num_classes = len(label_mapping)

    # Create reverse mapping for display
    label_mapping_inverse = {v: k for k, v in label_mapping.items()}

    print(f"\nNumber of classes: {num_classes}")
    print(f"Label mapping (first 10):")
    for i, (category, numeric_id) in enumerate(label_mapping.items()):
        if i < 10:  # Show only first 10 mappings
            count = np.sum(labels == numeric_id)
            print(f"  {category} -> {numeric_id}: {count} samples")
        elif i == 10:
            print("  ... (showing first 10 mappings only)")

    # Show sample data
    print("\nSample data:")
    for i in range(min(3, len(texts))):
        print(f"Sample {i+1}:")
        print(f"  Category: {label_mapping_inverse.get(labels[i], labels[i])} ({labels[i]})")
        print(f"  Text: {texts[i][:200]}...")
        print("-" * 80)

    return texts, labels, num_classes, label_mapping, label_mapping_inverse

def create_data_loaders(texts, labels, tokenizer, batch_size=16, max_length=256):
    """Create train, validation, and test data loaders"""
    # Split data: 70% train, 15% validation, 15% test
    X_temp, X_test, y_temp, y_test = train_test_split(
        texts, labels, test_size=0.15, random_state=42, stratify=labels
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 = 0.15
    )

    print(f"Data split:")
    print(f"  Train size: {len(X_train)}")
    print(f"  Validation size: {len(X_val)}")
    print(f"  Test size: {len(X_test)}")

    # Create datasets
    train_dataset = AmharicNewsDataset(X_train, y_train, tokenizer, max_length)
    val_dataset = AmharicNewsDataset(X_val, y_val, tokenizer, max_length)
    test_dataset = AmharicNewsDataset(X_test, y_test, tokenizer, max_length)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

def train_model(model, train_loader, val_loader, num_epochs=3, learning_rate=2e-5):
    """Train the model"""
    model = model.to(device)
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # Calculate total training steps
    total_steps = len(train_loader) * num_epochs

    # Create learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0,
        num_training_steps=total_steps
    )

    best_accuracy = 0
    train_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch + 1}/{num_epochs}')
        print('-' * 50)

        # Training phase
        model.train()
        total_train_loss = 0
        train_progress = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')

        for batch in train_progress:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_train_loss += loss.item()

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters
            optimizer.step()
            scheduler.step()

            # Update progress bar
            train_progress.set_postfix({'loss': loss.item()})

        # Calculate average training loss
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # Validation phase
        val_accuracy, val_loss = evaluate_model(model, val_loader)
        val_accuracies.append(val_accuracy)

        print(f'Training Loss: {avg_train_loss:.4f}')
        print(f'Validation Loss: {val_loss:.4f}')
        print(f'Validation Accuracy: {val_accuracy:.4f}')

        # Save best model
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'New best model saved with accuracy: {best_accuracy:.4f}')

    return train_losses, val_accuracies, best_accuracy

def evaluate_model(model, data_loader, return_predictions=False):
    """Evaluate the model"""
    model.eval()
    predictions = []
    true_labels = []
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Evaluating'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            total_loss += outputs.loss.item()

            # Get predictions
            logits = outputs.logits
            batch_predictions = torch.argmax(logits, dim=1)

            predictions.extend(batch_predictions.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    avg_loss = total_loss / len(data_loader)

    if return_predictions:
        return accuracy, avg_loss, predictions, true_labels
    return accuracy, avg_loss

def run_experiment(model_name, model_type, texts, labels, num_classes, label_mapping):
    """Run complete experiment for a given model"""
    print(f"\n{'='*60}")
    print(f"Training {model_name}")
    print(f"{'='*60}")

    # Load tokenizer and model
    print(f"Loading {model_name} tokenizer and model...")
    tokenizer = AutoTokenizer.from_pretrained(model_type)

    model = AutoModelForSequenceClassification.from_pretrained(
        model_type,
        num_labels=num_classes
    )

    # Create data loaders
    train_loader, val_loader, test_loader = create_data_loaders(
        texts, labels, tokenizer, batch_size=16, max_length=256
    )

    # Train model
    train_losses, val_accuracies, best_val_accuracy = train_model(
        model, train_loader, val_loader, num_epochs=3, learning_rate=2e-5
    )

    # Load best model and evaluate on test set
    print("Loading best model for testing...")
    model.load_state_dict(torch.load('best_model.pth', map_location=device))
    test_accuracy, test_loss, test_predictions, test_true_labels = evaluate_model(
        model, test_loader, return_predictions=True
    )

    print(f"\n{model_name} Results:")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print(f"Test Loss: {test_loss:.4f}")

    print("\nClassification Report:")
    # Create reverse mapping for readable class names
    reverse_mapping = {v: k for k, v in label_mapping.items()}
    target_names = [reverse_mapping[i] for i in range(num_classes)]
    print(classification_report(test_true_labels, test_predictions, target_names=target_names))

    # Plot confusion matrix
    plt.figure(figsize=(6, 5))
    cm = confusion_matrix(test_true_labels, test_predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=target_names,
                yticklabels=target_names)
    plt.title(f'{model_name} - Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    return {
        'model_name': model_name,
        'best_val_accuracy': best_val_accuracy,
        'test_accuracy': test_accuracy,
        'train_losses': train_losses,
        'val_accuracies': val_accuracies
    }

# MAIN EXECUTION
def main():
    # Set your CSV file path here
    CSV_PATH = "/content/drive/My Drive/Amharic News Dataset.csv"

    try:
        # Load and prepare data
        print("Loading your Amharic news dataset...")
        texts, labels, num_classes, label_mapping, label_mapping_inverse = load_and_prepare_data(CSV_PATH)

        # Define models to compare
        models_to_train = [
            {
                'name': 'mBERT',
                'type': 'bert-base-multilingual-cased'
            },
            {
                'name': 'XLM-RoBERTa Base',
                'type': 'xlm-roberta-base'
            }
        ]

        # Run experiments
        results = []
        for model_config in models_to_train:
            result = run_experiment(
                model_config['name'],
                model_config['type'],
                texts,
                labels,
                num_classes,
                label_mapping
            )
            results.append(result)

        # Print comparison results
        print(f"\n{'='*60}")
        print("FINAL RESULTS COMPARISON")
        print(f"{'='*60}")
        for result in results:
            print(f"{result['model_name']}:")
            print(f"  Best Validation Accuracy: {result['best_val_accuracy']:.4f}")
            print(f"  Test Accuracy: {result['test_accuracy']:.4f}")
            print()

        # Save results to file
        results_df = pd.DataFrame(results)
        results_df.to_csv('amharic_news_classification_results.csv', index=False)
        print("Results saved to 'amharic_news_classification_results.csv'")

        # Save label mapping for future use
        label_mapping_df = pd.DataFrame(list(label_mapping.items()), columns=['category', 'label_id'])
        label_mapping_df.to_csv('label_mapping.csv', index=False)
        print("Label mapping saved to 'label_mapping.csv'")

    except FileNotFoundError:
        print(f"Error: File '{CSV_PATH}' not found.")
        print("Please make sure the file path is correct and the file exists.")
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()

# Run the main function
if __name__ == "__main__":
    main()